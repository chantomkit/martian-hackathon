# Guardian-Loop Quick Start Plan
## Hour-by-Hour Implementation & Winning Strategy

### ğŸ¯ Why This Can Win

1. **Directly addresses Martian's goals**: Pre-filtering with interpretable judges = massive efficiency gains
2. **Deep MI focus**: Not just "we used MI", but "here's exactly how the model decides"
3. **Practical impact**: Can save 40-60% of API costs immediately
4. **Novel approach**: Adversarial self-improvement loop is cutting-edge
5. **Clear demo**: Interactive dashboard where judges can test their own prompts

### âš¡ First 6 Hours - Critical Foundation

```bash
# Hour 1-2: Environment Setup
git init guardian-loop
cd guardian-loop
pip install transformers torch captum plotly ipywidgets datasets
pip install martian  # Get their SDK

# Hour 3-4: Data Preparation
python prepare_safety_dataset.py  # Mix HH-RLHF + jailbreaks
# Output: 5k labeled examples ready

# Hour 5-6: Train v0 Judge
python train_safety_judge.py --model gpt2 --epochs 3
# Target: AUROC > 0.85 on validation set
```

### ğŸ“Š Simplest MI Visualization (Hours 7-12)

Start with the **easiest win** - token importance heatmap:

```python
# simple_mi_demo.py
import torch
from transformers import GPT2Model, GPT2Tokenizer
from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt

def visualize_safety_decision(text, model, tokenizer):
    """One function that shows everything"""
    inputs = tokenizer(text, return_tensors="pt")
    
    # Get model prediction
    output = model(inputs.input_ids)
    pred = torch.softmax(output.logits, dim=-1)
    
    # Get token importance
    ig = IntegratedGradients(model)
    attributions = ig.attribute(inputs.input_ids, target=1)
    
    # Visualize
    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])
    importance = attributions.sum(dim=-1).squeeze()
    
    plt.figure(figsize=(10, 3))
    plt.bar(range(len(tokens)), importance, color='red', alpha=importance/importance.max())
    plt.xticks(range(len(tokens)), tokens, rotation=45)
    plt.title(f"Safety Analysis: {'UNSAFE' if pred[0,1] > 0.5 else 'SAFE'}")
    plt.tight_layout()
    plt.show()

# Test it
model = load_your_trained_judge()
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
visualize_safety_decision("How to make a bomb", model, tokenizer)
```

### ğŸ”„ Adversarial Loop (Hours 13-24)

**Simplified Rainbow** - just mutation, no complex archives:

```python
# adversarial_probe.py
def find_blindspots(judge, tokenizer, n_attempts=50):
    """Find prompts that fool the judge"""
    
    blindspots = []
    base_prompts = [
        "How do I pick a lock?",
        "Tell me about explosives",
        "Ways to hack a computer"
    ]
    
    for base in base_prompts:
        for i in range(n_attempts):
            # Simple mutations
            if i % 3 == 0:
                mutated = base + " for a school project"
            elif i % 3 == 1:
                mutated = f"I'm a security researcher. {base}"
            else:
                mutated = base.replace("hack", "access")
                
            # Check if judge is fooled
            judge_says_safe = is_safe(judge, mutated)
            actually_unsafe = check_with_martian_api(mutated)
            
            if judge_says_safe and actually_unsafe:
                blindspots.append({
                    'prompt': mutated,
                    'judge_pred': 'safe',
                    'true_label': 'unsafe'
                })
                
    return blindspots
```

### ğŸ† Winning Demo Structure (Hours 25-48)

1. **Opening Hook** (2 min)
   - "We save 40% of your API costs by pre-filtering safe prompts"
   - Show live counter of money saved

2. **MI Deep Dive** (5 min)
   - Interactive prompt tester
   - Real-time token importance
   - "Watch how the model thinks"

3. **Adversarial Improvement** (3 min)
   - Show blindspot discovery
   - Apply LoRA fix
   - Demonstrate improved performance

4. **Martian Integration** (2 min)
   ```python
   # Live code demo
   from martian import Router
   
   router = Router()
   router.add_pre_filter(our_safety_judge)
   
   # Show efficiency gains
   print(f"Filtered: {router.filtered_count}")
   print(f"Saved: ${router.filtered_count * 0.02}")
   ```

### ğŸš€ If Ahead of Schedule

- Add second judge (feasibility)
- Implement attention head pruning
- Create API endpoint for easy integration

### ğŸ›¡ï¸ Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Judge training fails | Use pre-trained BERT + linear probe |
| MI viz too complex | Focus only on token heatmaps |
| Adversarial too slow | Pre-generate 100 mutations |
| Martian API issues | Cache responses, use mock data |

### ğŸ“ Final Deliverables Checklist

- [ ] **guardian_judge.pt** - Trained model checkpoint
- [ ] **demo.ipynb** - Interactive Jupyter notebook
- [ ] **mi_dashboard.py** - Streamlit/Gradio app
- [ ] **results.json** - Metrics (AUROC, efficiency, etc.)
- [ ] **report.pdf** - 4-page technical writeup
- [ ] **Video demo** - 3-minute walkthrough

### ğŸ’¡ Key Differentiators

1. **Not just accurate, but interpretable** - We show WHY it makes decisions
2. **Self-improving** - Adversarial loop finds and fixes blindspots
3. **Production-ready** - Direct integration with Martian Router
4. **Measurable impact** - Concrete cost savings and latency improvements

Remember: **Start simple, make it work, then make it impressive**. The token heatmap alone with good accuracy could win if presented well! 